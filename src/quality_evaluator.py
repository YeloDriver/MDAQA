"""
Quality evaluation module for MDAQA project.
"""
import json
from typing import Dict, List, Any, Optional
from .llm_client import LLMClient
from .data_loader import DataLoader


class QualityEvaluator:
    """Evaluates the quality of generated questions and answers."""
    
    def __init__(self, config_path: str = "config/config.yaml"):
        """Initialize quality evaluator."""
        self.llm_client = LLMClient(config_path)
        self.data_loader = DataLoader(config_path)
    
    def evaluate_question(self, question: str, ground_truth: str, 
                         support_papers: List[str]) -> List[Dict[str, Any]]:
        """
        Evaluate a question by comparing ground truth with individual paper answers.
        
        Args:
            question: The question to evaluate
            ground_truth: The ground truth answer
            support_papers: List of supporting paper arXiv IDs
            
        Returns:
            List of evaluation results for each supporting paper
        """
        # Load paper contents
        paper_contents = self._get_paper_contents(support_papers)
        
        evaluations = []
        
        for i, (paper_id, content) in enumerate(zip(support_papers, paper_contents)):
            if content is None:
                continue
            
            # Generate evaluation prompt
            evaluation_prompt = self._create_evaluation_prompt(
                question, paper_id, ground_truth, content
            )
            
            try:
                # Generate evaluation
                response = self.llm_client.generate_with_retry(
                    content,  # Use paper content as system context
                    evaluation_prompt
                )
                
                evaluation = {
                    "support_paper": paper_id,
                    "evaluation": response
                }
                evaluations.append(evaluation)
                
            except Exception as e:
                print(f"Error evaluating paper {paper_id}: {e}")
                continue
        
        return evaluations
    
    def _get_paper_contents(self, arxiv_ids: List[str]) -> List[Optional[str]]:
        """Load content for multiple papers."""
        contents = []
        spiqa_path = self.data_loader.config['data']['spiqa_path']
        min_size = self.data_loader.config['processing']['min_file_size'] * 1024
        max_size = self.data_loader.config['processing']['max_file_size'] * 1024
        
        for arxiv_id in arxiv_ids:
            content = self.data_loader._load_single_paper(arxiv_id, spiqa_path, min_size, max_size)
            if content:
                formatted_content = f"**arxiv_id**: {arxiv_id}\n**content**: {content}\n\n"
                contents.append(formatted_content)
            else:
                contents.append(None)
        
        return contents
    
    def _create_evaluation_prompt(self, question: str, paper_id: str, 
                                 ground_truth: str, paper_content: str) -> str:
        """Create evaluation prompt for comparing answers."""
        prompt = f"""Based on the following question and a set of corresponding scientific papers:
Question: {question}
Directly respond to the question based on the paper {paper_id} as model answer. Don't provide any additional information. You must provide a concise answer in one sentence only.
Compare the model answer with the ground truth answer based on the provided papers: 
Ground truth answer: {ground_truth}

Determine which answer is better and why.
The output should be a JSON object with the following format:
"""
        prompt += '{"model answer": "Answer generated by the indicated paper", "better_answer": "choose between [model_answer, ground truth answer, neither, equal]. Neither means both answers have significant issues for the question. Equal means both answers convey essentially the same key points.", "reason_for_better_answer": "Briefly shows reason why better"}'
        
        return prompt
    
    def evaluate_dataset(self) -> None:
        """Evaluate the complete generated dataset."""
        # Load generated QA data
        qa_path = self.data_loader.config['data']['qa_output']
        qa_data = self.data_loader.load_progress(qa_path)
        
        if not qa_data:
            raise ValueError(f"No QA data found at {qa_path}. Please run question generation first.")
        
        # Load existing evaluation progress
        evaluation_path = self.data_loader.config['data']['evaluation_output']
        evaluations = self.data_loader.load_progress(evaluation_path)
        
        # Convert to list if it's a dict (for compatibility)
        if isinstance(evaluations, dict):
            evaluations = []
        
        # Get list of already evaluated questions
        evaluated_questions = [item["question"] for item in evaluations]
        
        print(f"Starting evaluation of {len(qa_data)} communities...")
        
        total_questions = 0
        evaluated_count = 0
        
        for community_id, community_data in qa_data.items():
            for question_data in community_data["questions"]:
                question = question_data["Q"]
                total_questions += 1
                
                # Skip if already evaluated
                if question in evaluated_questions:
                    evaluated_count += 1
                    continue
                
                print(f"Evaluating question: {question[:100]}...")
                
                ground_truth = question_data["A"]
                support_papers = question_data["Support"]
                
                # Evaluate the question
                question_evaluations = self.evaluate_question(
                    question, ground_truth, support_papers
                )
                
                # Create evaluation record
                evaluation_record = {
                    "question": question,
                    "ground_truth": ground_truth,
                    "support": support_papers,
                    "community": community_id,
                    "evaluation": question_evaluations
                }
                
                evaluations.append(evaluation_record)
                evaluated_count += 1
                
                # Save progress
                self.data_loader.save_progress(evaluations, evaluation_path)
                
                print(f"Progress: {evaluated_count}/{total_questions} questions evaluated")
        
        print(f"Evaluation complete. Results saved to {evaluation_path}")
    
    def generate_final_dataset(self) -> None:
        """Generate the final MDAQA dataset in the required format."""
        # Load QA data
        qa_path = self.data_loader.config['data']['qa_output']
        qa_data = self.data_loader.load_progress(qa_path)
        
        if not qa_data:
            raise ValueError(f"No QA data found at {qa_path}")
        
        final_dataset = []
        question_id = 0
        
        for community_id, community_data in qa_data.items():
            for question_data in community_data["questions"]:
                # Create final format entry
                entry = {
                    "id": question_id,
                    "question": question_data["Q"],
                    "answer": question_data["A"],
                    "support": question_data["Support"]
                }
                
                final_dataset.append(entry)
                question_id += 1
        
        # Save final dataset
        final_path = self.data_loader.config['data']['final_dataset']
        self.data_loader.save_progress(final_dataset, final_path)
        
        print(f"Final dataset generated with {len(final_dataset)} questions.")
        print(f"Saved to: {final_path}")


def main():
    """Main function to run quality evaluation."""
    evaluator = QualityEvaluator()
    
    print("Starting quality evaluation...")
    evaluator.evaluate_dataset()
    
    print("Generating final dataset...")
    evaluator.generate_final_dataset()


if __name__ == "__main__":
    main()
