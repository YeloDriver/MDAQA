"""
Question generation module for MDAQA project.
"""
import json
import re
from typing import Dict, List, Any, Optional
from .llm_client import LLMClient
from .data_loader import DataLoader


class QuestionGenerator:
    """Generates multi-document questions from academic paper communities."""
    
    def __init__(self, config_path: str = "config/config.yaml"):
        """Initialize question generator."""
        self.llm_client = LLMClient(config_path)
        self.data_loader = DataLoader(config_path)
        
        self.system_prompt = """You are a professor capable of reading and synthesizing multiple academic documents. You have been given several small collections of literature generated by community detection. These documents may share a common research theme, may cite each other's methods, or have other interrelated clues. Your tasks include the following:
1. From each given literature collection (i.e., small community), select at least two articles with interconnections.
2. Based on the selected articles, propose at least one complex question that requires cross-referencing multiple documents to answer.
   - This question must necessitate reading more than one document to arrive at a complete answer. In other words, if someone reads only one of the documents, they would not be able to get the full solution.
3. Using the selected literature, answer the question you posed. Both the question and the answer must be firmly grounded in the content of those documents.
4. Both the question and the answer should demonstrate depth, highlighting the connections or contrasts among multiple documents instead of restating conclusions from any single source.
   - The goal is to showcase the complexity and value of integrating multi-document information."""
    
    def generate_questions_for_community(self, community: Dict[str, Any], 
                                       semantic_mapping: Dict[str, Dict[str, str]]) -> Optional[Dict[str, Any]]:
        """
        Generate questions for a single community.
        
        Args:
            community: Community data with papers
            semantic_mapping: Mapping from semantic IDs to arXiv IDs
            
        Returns:
            Dictionary with papers and generated questions, or None if failed
        """
        # Extract arXiv IDs and titles
        arxiv_ids = []
        for paper_id in community["papers"]:
            if paper_id in semantic_mapping:
                paper_info = semantic_mapping[paper_id]
                arxiv_ids.append((paper_info["arxiv_id"], paper_info["title"]))
        
        # Load paper content
        contents, valid_arxiv_ids = self.data_loader.get_paper_content(arxiv_ids)
        
        if contents is None:
            return None
        
        # Generate questions
        user_prompt = self._create_user_prompt(contents)
        
        try:
            # Use JSON format for OpenAI, regular format for others
            use_json = self.llm_client.config['llm']['provider'].lower() in ["openai", "azure_openai"]
            response = self.llm_client.generate_with_retry(
                self.system_prompt, 
                user_prompt, 
                use_json_format=use_json
            )
            
            # Parse and clean the response
            questions = self._parse_response(response)
            
            # Filter out single-support questions
            filtered_questions = self._filter_single_support(questions)
            
            return {
                "papers": valid_arxiv_ids,
                "questions": filtered_questions
            }
            
        except Exception as e:
            print(f"Error generating questions: {e}")
            return None
    
    def _create_user_prompt(self, contents: str) -> str:
        """Create the user prompt for question generation."""
        prompt = f"""**Task Context**
Analyze scientific literature communities to generate complex questions requiring synthesis of multiple documents. Each question must:
1. Involve at least 2 papers with deep interconnections
2. Cover question types including but not limited to:
- Methodological comparison and critique
- Contradictory conclusion analysis
- Technological evolution tracing
- Multidimensional evaluation
- Hypothesis validation pathways

**Community Information**
{contents}

**Processing Pipeline**
1. Select at least two papers with deep interconnections.

2. Find the connection between these papers, identify Problem Spaces:
- Method contrast: CNN in Paper X vs Transformer in Paper Y
- Conclusion conflict: p<0.01 in Paper Z vs p>0.05 in Paper W
- Technical progression: Baseline in Paper A â†’ Optimized solution in Paper D

3. Based on the first two steps, generate questions following the rules:
- Avoid asking simple or definitional questions.
- Avoid asking questions like "How do different approaches...", "What are the key challenges...", "What are the key differences..."
- The questions should be in one sentence only, they should not consist of more than one question.
- The questions should not contain the titles or method name, don't use phrases like 'as discussed in the papers'.
- Ensure that the answers are concise, accurate, and directly related to the corresponding question.
- Do not generate any information that does not appear in the original documents, nor make unsupported inferences.

Repeat the process to generate questions as much as possible.
"""
        
        prompt += '\nReturn question Q, answer A, all the reference paper ids S in a json format, don\'t return any other words.\n{"data": [{"Q": ,"A": , "Reason": , "Support": [paper_id_1 ,..., paper_id_n]}]}'
        
        return prompt
    
    def _parse_response(self, response: str) -> List[Dict[str, Any]]:
        """Parse the LLM response to extract questions."""
        # For non-JSON responses, extract JSON using regex
        if not response.strip().startswith('{'):
            json_pattern = r"\{.*\}"
            json_match = re.search(json_pattern, response, re.DOTALL)
            
            if json_match:
                json_string = json_match.group(0)
                # Clean up common JSON formatting issues
                json_string = json_string.replace(",}", "}")
                json_string = json_string.replace(",]", "]")
                json_string = json_string.replace(",\n}", "}")
                json_string = json_string.replace(",\n]", "]")
                response = json_string
            else:
                raise ValueError("No JSON data found in response")
        
        # Parse JSON
        try:
            data = json.loads(response)
        except json.JSONDecodeError:
            # Try adding missing closing brackets
            response += "]}"
            data = json.loads(response)
        
        return data.get("data", [])
    
    def _filter_single_support(self, questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Filter out questions that only reference a single paper."""
        filtered_questions = []
        
        for question in questions:
            support_papers = []
            for support in question.get("Support", []):
                if support not in support_papers:
                    support_papers.append(support)
            
            # Only keep questions with multiple supporting papers
            if len(support_papers) > 1:
                filtered_questions.append(question)
        
        return filtered_questions
    
    def generate_dataset(self) -> None:
        """Generate the complete MDAQA dataset."""
        # Load input data
        communities = self.data_loader.load_community_data()
        semantic_mapping = self.data_loader.load_semantic_mapping()
        
        # Load existing progress
        qa_path = self.data_loader.config['data']['qa_output']
        qa_data = self.data_loader.load_progress(qa_path)
        
        print(f"Processing {len(communities)} communities...")
        
        for i, community in enumerate(communities):
            community_id = str(community["community_id"])
            
            # Skip if already processed
            if community_id in qa_data:
                continue
            
            print(f"Processing community {community_id} ({i+1}/{len(communities)})")
            
            result = self.generate_questions_for_community(community, semantic_mapping)
            
            if result:
                qa_data[community_id] = result
                self.data_loader.save_progress(qa_data, qa_path)
                print(f"Generated {len(result['questions'])} questions for community {community_id}")
            else:
                print(f"Failed to generate questions for community {community_id}")
        
        print(f"Question generation complete. Results saved to {qa_path}")


def main():
    """Main function to run question generation."""
    generator = QuestionGenerator()
    generator.generate_dataset()


if __name__ == "__main__":
    main()
